{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KafkaException",
     "evalue": "KafkaError{code=UNKNOWN_TOPIC_OR_PART,val=3,str=\"Subscribed topic not available: my-topic: Broker: Unknown topic or partition\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKafkaException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m msg\u001b[38;5;241m.\u001b[39merror():\n\u001b[0;32m---> 20\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m KafkaException(msg\u001b[38;5;241m.\u001b[39merror())\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReceived message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;241m.\u001b[39mvalue()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Закрытие консюмера\u001b[39;00m\n",
      "\u001b[0;31mKafkaException\u001b[0m: KafkaError{code=UNKNOWN_TOPIC_OR_PART,val=3,str=\"Subscribed topic not available: my-topic: Broker: Unknown topic or partition\"}"
     ]
    }
   ],
   "source": [
    "## Читаем с автоматическим офсетом\n",
    "from confluent_kafka import Consumer, KafkaException\n",
    "# Конфигурация консюмера\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'my-group',\n",
    "    'auto.offset.reset': 'earliest',  # Чтение с самого начала, если офсеты не найдены\n",
    "    'enable.auto.commit': True        # Включение автоматического коммита офсетов\n",
    "}\n",
    "# Создание консюмера\n",
    "consumer = Consumer(conf)\n",
    "# Подписка на топик\n",
    "consumer.subscribe(['my-topic'])\n",
    "try:\n",
    "    for _ in range(5):\n",
    "        msg = consumer.poll(timeout=1.0)  # Ожидание новых сообщений  \n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            raise KafkaException(msg.error())\n",
    "        print(f'Received message: {msg.value().decode(\"utf-8\")}')\n",
    "        \n",
    "finally:\n",
    "    # Закрытие консюмера\n",
    "    consumer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Читаем с РУЧНЫМ офсетом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received message: qwe\n",
      "Received message: asd\n",
      "Received message: zxc\n",
      "Received message: qwf\n",
      "Received message: asd\n",
      "Received message: hello\n",
      "Received message: gr1\n",
      "Received message: asd\n",
      "Received message: asd\n",
      "Received message: asdzxv\n",
      "Received message: 123\n",
      "Received message: 123\n",
      "Received message: 123\n",
      "Received message: я1\n",
      "Received message: Hello Kafka 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from confluent_kafka import Consumer, KafkaException\n",
    "# Конфигурация консюмера\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'my-group-222222222', ## другая группа\n",
    "    'auto.offset.reset': 'earliest',  # Чтение с самого начала, если офсеты не найдены\n",
    "    'enable.auto.commit': False        # \n",
    "}\n",
    "# Создание консюмера\n",
    "consumer = Consumer(conf)\n",
    "# Подписка на топик\n",
    "consumer.subscribe(['my-topic'])\n",
    "try:\n",
    "    for _ in range(15):\n",
    "        msg = consumer.poll(timeout=1.0)  # Ожидание новых сообщений  \n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            raise KafkaException(msg.error())\n",
    "        print(f'Received message: {msg.value().decode(\"utf-8\")}')\n",
    "        \n",
    "finally:\n",
    "    # Закрытие консюмера\n",
    "    consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтение офсетов для конкретной группы консумеров\n",
    "Для чтения текущих офсетов для группы консумеров можно использовать метод committed(partitions). Этот метод возвращает список офсетов для указанных разделов (partitions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition: 0, Offset: 24\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Consumer, TopicPartition\n",
    "\n",
    "# Конфигурация консюмера\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'my-group',\n",
    "    'auto.offset.reset': 'earliest',\n",
    "    'enable.auto.commit': False\n",
    "}\n",
    "\n",
    "consumer = Consumer(conf)\n",
    "consumer.subscribe(['my-topic'])\n",
    "\n",
    "# Получение текущих офсетов для группы консумеров по разделам топика\n",
    "partitions = [TopicPartition('my-topic', partition=0)]\n",
    "offsets = consumer.committed(partitions)\n",
    "\n",
    "for tp in offsets:\n",
    "    print(f'Partition: {tp.partition}, Offset: {tp.offset}')\n",
    "\n",
    "consumer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message delivered to my-topic [0] at offset 39\n",
      "Message delivered to my-topic [0] at offset 40\n",
      "Message delivered to my-topic [0] at offset 41\n",
      "Message delivered to my-topic [0] at offset 42\n",
      "finally\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Producer\n",
    "\n",
    "# Функция для обработки успешной доставки сообщений\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print(f'Message delivery failed: {err}')\n",
    "    else:\n",
    "        print(f'Message delivered to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}')\n",
    "\n",
    "# Конфигурация продюсера\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',  # Адрес Kafka брокера\n",
    "    'client.id': 'my-producer'                # Идентификатор клиента\n",
    "}\n",
    "\n",
    "# Создание экземпляра продюсера\n",
    "producer = Producer(conf)\n",
    "\n",
    "# Отправка сообщений\n",
    "try:\n",
    "    for i in range(4):\n",
    "        key = f'key-{i}'                # Ключ сообщения (опционально)\n",
    "        value = f'Hello Kafka {i}'      # Значение сообщения\n",
    "        producer.produce('my-topic', key=key, value=value, callback=delivery_report)\n",
    "\n",
    "        # Обработка асинхронных операций\n",
    "        producer.poll(0)  # Проверка на наличие ошибок и вызов обратных функций\n",
    "\n",
    "    # Ожидание отправки всех сообщений\n",
    "    producer.flush()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Error while producing: {e}')\n",
    "\n",
    "finally:\n",
    "    # Закрытие продюсера\n",
    "    print('finally')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exactly-once semantics, EOS\n",
    "\n",
    " idempotent producer и настроек для транзакций\n",
    " Конфигурация продюсера:\n",
    "\n",
    "enable.idempotence: True: Включает идемпотентный режим для продюсера.\n",
    "transactional.id: Уникальный идентификатор для транзакций. Он должен быть уникальным для каждого продюсера в вашем приложении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%4|1729585021.552|GETPID|my-producer#producer-22| [thrd:main]: Failed to acquire transactional PID from broker TxnCoordinator/1: Broker: Not coordinator: retrying\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message delivered to my-topic [0] at offset 34\n",
      "Message delivered to my-topic [0] at offset 35\n",
      "Message delivered to my-topic [0] at offset 36\n",
      "Message delivered to my-topic [0] at offset 37\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Producer\n",
    "\n",
    "# Функция для обработки успешной доставки сообщений\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print(f'Message delivery failed: {err}')\n",
    "    else:\n",
    "        print(f'Message delivered to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}')\n",
    "\n",
    "# Конфигурация продюсера\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',  # Адрес Kafka брокера\n",
    "    'client.id': 'my-producer',              # Идентификатор клиента\n",
    "    'enable.idempotence': True,               # Включение идемпотентного режима\n",
    "    'acks': 'all',                            # Подтверждение от всех реплик\n",
    "    'retries': 5,                             # Количество попыток повторной отправки\n",
    "    'transactional.id': 'my-transactional-id'  # Идентификатор транзакции\n",
    "}\n",
    "\n",
    "# Создание экземпляра продюсера\n",
    "producer = Producer(conf)\n",
    "\n",
    "# Инициализация транзакции\n",
    "producer.init_transactions()\n",
    "\n",
    "try:\n",
    "    # Начало транзакции\n",
    "    producer.begin_transaction()\n",
    "\n",
    "    # Отправка сообщений\n",
    "    for i in range(4):\n",
    "        key = f'key-{i}'                # Ключ сообщения (опционально)\n",
    "        value = f'Hello Kafka {i}'      # Значение сообщения\n",
    "        producer.produce('my-topic', key=key, value=value, callback=delivery_report)\n",
    "\n",
    "        # Обработка асинхронных операций\n",
    "        producer.poll(0)  # Проверка на наличие ошибок и вызов обратных функций\n",
    "\n",
    "    # Завершение транзакции\n",
    "    producer.commit_transaction()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Error while producing: {e}')\n",
    "    # Отмена транзакции в случае ошибки\n",
    "    producer.abort_transaction()\n",
    "\n",
    "finally:\n",
    "    # Ожидание отправки всех сообщений\n",
    "    producer.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my-avro-topic-value', 'my-topic-value']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "response = requests.get(\"http://localhost:8081/subjects\")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регистрация новой схемы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Схема зарегистрирована с ID: 1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = 'http://localhost:8081/subjects/my-topic-value/versions'\n",
    "schema = {\n",
    "    \"schema\": json.dumps({\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"User\",\n",
    "        \"fields\": [\n",
    "            {\"name\": \"name\", \"type\": \"string\"},\n",
    "            {\"name\": \"age\", \"type\": \"int\"}\n",
    "        ]\n",
    "    })\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=schema)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    schema_id = response.json().get('id')\n",
    "    print('Схема зарегистрирована с ID:', schema_id)\n",
    "else:\n",
    "    print('Ошибка при регистрации схемы:', response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Полученная схема: {\"type\":\"record\",\"name\":\"User\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"age\",\"type\":\"int\"}]}\n"
     ]
    }
   ],
   "source": [
    "schema_id = 1  # Замените на нужный ID\n",
    "url = f'http://localhost:8081/schemas/ids/{schema_id}'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    schema = response.json().get('schema')\n",
    "    print('Полученная схема:', schema)\n",
    "else:\n",
    "    print('Ошибка при получении схемы:', response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AVRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject': 'my-topic-value',\n",
       " 'version': 1,\n",
       " 'id': 1,\n",
       " 'schema': '{\"type\":\"record\",\"name\":\"User\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"age\",\"type\":\"int\"}]}'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(\"http://localhost:8081/subjects/my-topic-value/versions/latest\")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class User(object):\n",
    "    def __init__(self, name, address, favorite_number, favorite_color):\n",
    "        self.name = name\n",
    "        self.favorite_number = favorite_number\n",
    "        self.favorite_color = favorite_color\n",
    "        # address should not be serialized, see user_to_dict()\n",
    "        self._address = address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_to_dict(user, ctx):\n",
    "    # User._address must not be serialized; omit from dict\n",
    "    return dict(name=user.name,\n",
    "                favorite_number=user.favorite_number,\n",
    "                favorite_color=user.favorite_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delivery_report(err, msg):\n",
    " \n",
    "    if err is not None:\n",
    "        print(\"Delivery failed for User record {}: {}\".format(msg.key(), err))\n",
    "        return\n",
    "    print('User record {} successfully produced to {} [{}] at offset {}'.format(\n",
    "        msg.key(), msg.topic(), msg.partition(), msg.offset()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producing user records to topic my-topic. ^C to exit.\n"
     ]
    },
    {
     "ename": "SchemaRegistryError",
     "evalue": "Schema being registered is incompatible with an earlier schema for subject \"my-topic-value\", details: [{errorType:'READER_FIELD_MISSING_DEFAULT_VALUE', description:'The field 'favorite_number' at path '/fields/1' in the new schema has no default value and is missing in the old schema', additionalInfo:'favorite_number'}, {errorType:'READER_FIELD_MISSING_DEFAULT_VALUE', description:'The field 'favorite_color' at path '/fields/2' in the new schema has no default value and is missing in the old schema', additionalInfo:'favorite_color'}, {oldSchemaVersion: 1}, {oldSchema: '{\"type\":\"record\",\"name\":\"User\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"age\",\"type\":\"int\"}]}'}, {compatibility: 'BACKWARD'}] io.confluent.kafka.schemaregistry.rest.exceptions.RestIncompatibleSchemaException: Schema being registered is incompatible with an earlier schema for subject \"my-topic-value\", details: [{errorType:'READER_FIELD_MISSING_DEFAULT_VALUE', description:'The field 'favorite_number' at path '/fields/1' in the new schema has no default value and is missing in the old schema', additionalInfo:'favorite_number'}, {errorType:'READER_FIELD_MISSING_DEFAULT_VALUE', description:'The field 'favorite_color' at path '/fields/2' in the new schema has no default value and is missing in the old schema', additionalInfo:'favorite_color'}, {oldSchemaVersion: 1}, {oldSchema: '{\"type\":\"record\",\"name\":\"User\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"age\",\"type\":\"int\"}]}'}, {compatibility: 'BACKWARD'}]\nio.confluent.kafka.schemaregistry.rest.exceptions.RestIncompatibleSchemaException: Schema being registered is incompatible with an earlier schema for subject \"my-topic-value\", details: [{errorType:'READER_FIELD_MISSING_DEFAULT_VALUE', description:'The field 'favorite_number' at path '/fields/1' in the new schema has no default value and is missing in the old schema', additionalInfo:'favorite_number'}, {errorType:'READER_FIELD_MISSING_DEFAULT_VALUE', description:'The field 'favorite_color' at path '/fields/2' in the new schema has no default value and is missing in the old schema', additionalInfo:'favorite_color'}, {oldSchemaVersion: 1}, {oldSchema: '{\"type\":\"record\",\"name\":\"User\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"age\",\"type\":\"int\"}]}'}, {compatibility: 'BACKWARD'}]\n\tat io.confluent.kafka.schemaregistry.rest.exceptions.Errors.incompatibleSchemaException(Errors.java:134)\n\tat io.confluent.kafka.schemaregistry.rest.resources.SubjectVersionsResource.register(SubjectVersionsResource.java:431)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)\n\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:134)\n\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:177)\n\tat org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$VoidOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:159)\n\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:81)\n\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)\n\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)\n\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)\n\tat org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)\n\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)\n\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:292)\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:274)\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:244)\n\tat org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)\n\tat org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)\n\tat org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:684)\n\tat org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)\n\tat org.glassfish.jersey.servlet.ServletContainer.serviceImpl(ServletContainer.java:378)\n\tat org.glassfish.jersey.servlet.ServletContainer.doFilter(ServletContainer.java:553)\n\tat org.glassfish.jersey.servlet.ServletContainer.doFilter(ServletContainer.java:494)\n\tat org.glassfish.jersey.servlet.ServletContainer.doFilter(ServletContainer.java:431)\n\tat org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n\tat org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:552)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\n\tat org.eclipse.jetty.server.handler.RequestLogHandler.handle(RequestLogHandler.java:54)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n\tat io.confluent.kafka.schemaregistry.rest.RequestIdHandler.handle(RequestIdHandler.java:51)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)\n\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)\n\tat org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:181)\n\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n\tat org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:772)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: io.confluent.kafka.schemaregistry.exceptions.IncompatibleSchemaException: [{errorType:'READER_FIELD_MISSING_DEFAULT_VALUE', description:'The field 'favorite_number' at path '/fields/1' in the new schema has no default value and is missing in the old schema', additionalInfo:'favorite_number'}, {errorType:'READER_FIELD_MISSING_DEFAULT_VALUE', description:'The field 'favorite_color' at path '/fields/2' in the new schema has no default value and is missing in the old schema', additionalInfo:'favorite_color'}, {oldSchemaVersion: 1}, {oldSchema: '{\"type\":\"record\",\"name\":\"User\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"age\",\"type\":\"int\"}]}'}, {compatibility: 'BACKWARD'}]\n\tat io.confluent.kafka.schemaregistry.storage.KafkaSchemaRegistry.register(KafkaSchemaRegistry.java:739)\n\tat io.confluent.kafka.schemaregistry.storage.KafkaSchemaRegistry.registerOrForward(KafkaSchemaRegistry.java:870)\n\tat io.confluent.kafka.schemaregistry.rest.resources.SubjectVersionsResource.register(SubjectVersionsResource.java:412)\n\t... 61 more\n (HTTP status code 409, SR code 409)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSchemaRegistryError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 44\u001b[0m\n\u001b[1;32m     37\u001b[0m     user_favorite_color \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter favorite color: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m     user \u001b[38;5;241m=\u001b[39m User(name\u001b[38;5;241m=\u001b[39muser_name,\n\u001b[1;32m     39\u001b[0m                 address\u001b[38;5;241m=\u001b[39muser_address,\n\u001b[1;32m     40\u001b[0m                 favorite_color\u001b[38;5;241m=\u001b[39muser_favorite_color,\n\u001b[1;32m     41\u001b[0m                 favorite_number\u001b[38;5;241m=\u001b[39muser_favorite_number)\n\u001b[1;32m     42\u001b[0m     producer\u001b[38;5;241m.\u001b[39mproduce(topic\u001b[38;5;241m=\u001b[39mtopic,\n\u001b[1;32m     43\u001b[0m                         key\u001b[38;5;241m=\u001b[39mstring_serializer(\u001b[38;5;28mstr\u001b[39m(uuid4())),\n\u001b[0;32m---> 44\u001b[0m                         value\u001b[38;5;241m=\u001b[39m\u001b[43mavro_serializer\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSerializationContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMessageField\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVALUE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     45\u001b[0m                         on_delivery\u001b[38;5;241m=\u001b[39mdelivery_report)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/confluent_kafka/schema_registry/avro.py:289\u001b[0m, in \u001b[0;36mAvroSerializer.__call__\u001b[0;34m(self, obj, ctx)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;66;03m# Check to ensure this schema has been registered under subject_name.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_auto_register:\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;66;03m# The schema name will always be the same. We can't however register\u001b[39;00m\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;66;03m# a schema without a subject so we set the schema_id here to handle\u001b[39;00m\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;66;03m# the initial registration.\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m                                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m                                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_schemas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    293\u001b[0m         registered_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry\u001b[38;5;241m.\u001b[39mlookup_schema(subject,\n\u001b[1;32m    294\u001b[0m                                                          \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema,\n\u001b[1;32m    295\u001b[0m                                                          \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_schemas)\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/confluent_kafka/schema_registry/schema_registry_client.py:421\u001b[0m, in \u001b[0;36mSchemaRegistryClient.register_schema\u001b[0;34m(self, subject_name, schema, normalize_schemas)\u001b[0m\n\u001b[1;32m    415\u001b[0m     request[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mschemaType\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m schema\u001b[38;5;241m.\u001b[39mschema_type\n\u001b[1;32m    416\u001b[0m     request[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreferences\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: ref\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    417\u001b[0m                               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m'\u001b[39m: ref\u001b[38;5;241m.\u001b[39msubject,\n\u001b[1;32m    418\u001b[0m                               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m'\u001b[39m: ref\u001b[38;5;241m.\u001b[39mversion}\n\u001b[1;32m    419\u001b[0m                              \u001b[38;5;28;01mfor\u001b[39;00m ref \u001b[38;5;129;01min\u001b[39;00m schema\u001b[38;5;241m.\u001b[39mreferences]\n\u001b[0;32m--> 421\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rest_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msubjects/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/versions?normalize=\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_urlencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubject_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_schemas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m schema_id \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mset(schema_id, schema, subject_name)\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/confluent_kafka/schema_registry/schema_registry_client.py:128\u001b[0m, in \u001b[0;36m_RestClient.post\u001b[0;34m(self, url, body, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, body, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/confluent_kafka/schema_registry/schema_registry_client.py:175\u001b[0m, in \u001b[0;36m_RestClient.send_request\u001b[0;34m(self, url, method, body, query)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m299\u001b[39m:\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SchemaRegistryError(response\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[1;32m    176\u001b[0m                               response\u001b[38;5;241m.\u001b[39mjson()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror_code\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    177\u001b[0m                               response\u001b[38;5;241m.\u001b[39mjson()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# Schema Registry may return malformed output when it hits unexpected errors\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n",
      "\u001b[0;31mSchemaRegistryError\u001b[0m: Schema being registered is incompatible with an earlier schema for subject \"my-topic-value\", details: [{errorType:'READER_FIELD_MISSING_DEFAULT_VALUE', description:'The field 'favorite_number' at path '/fields/1' in the new schema has no default value and is missing in the old schema', additionalInfo:'favorite_number'}, {errorType:'READER_FIELD_MISSING_DEFAULT_VALUE', description:'The field 'favorite_color' at path '/fields/2' in the new schema has no default value and is missing in the old schema', additionalInfo:'favorite_color'}, {oldSchemaVersion: 1}, {oldSchema: '{\"type\":\"record\",\"name\":\"User\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"age\",\"type\":\"int\"}]}'}, {compatibility: 'BACKWARD'}] io.confluent.kafka.schemaregistry.rest.exceptions.RestIncompatibleSchemaException: Schema being registered is incompatible with an earlier schema for subject \"my-topic-value\", details: [{errorType:'READER_FIELD_MISSING_DEFAULT_VALUE', description:'The field 'favorite_number' at path '/fields/1' in the new schema has no default value and is missing in the old schema', additionalInfo:'favorite_number'}, {errorType:'READER_FIELD_MISSING_DEFAULT_VALUE', description:'The field 'favorite_color' at path '/fields/2' in the new schema has no default value and is missing in the old schema', additionalInfo:'favorite_color'}, {oldSchemaVersion: 1}, {oldSchema: '{\"type\":\"record\",\"name\":\"User\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"age\",\"type\":\"int\"}]}'}, {compatibility: 'BACKWARD'}]\nio.confluent.kafka.schemaregistry.rest.exceptions.RestIncompatibleSchemaException: Schema being registered is incompatible with an earlier schema for subject \"my-topic-value\", details: [{errorType:'READER_FIELD_MISSING_DEFAULT_VALUE', description:'The field 'favorite_number' at path '/fields/1' in the new schema has no default value and is missing in the old schema', additionalInfo:'favorite_number'}, {errorType:'READER_FIELD_MISSING_DEFAULT_VALUE', description:'The field 'favorite_color' at path '/fields/2' in the new schema has no default value and is missing in the old schema', additionalInfo:'favorite_color'}, {oldSchemaVersion: 1}, {oldSchema: '{\"type\":\"record\",\"name\":\"User\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"age\",\"type\":\"int\"}]}'}, {compatibility: 'BACKWARD'}]\n\tat io.confluent.kafka.schemaregistry.rest.exceptions.Errors.incompatibleSchemaException(Errors.java:134)\n\tat io.confluent.kafka.schemaregistry.rest.resources.SubjectVersionsResource.register(SubjectVersionsResource.java:431)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)\n\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:134)\n\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:177)\n\tat org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$VoidOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:159)\n\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:81)\n\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)\n\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)\n\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)\n\tat org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)\n\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)\n\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:292)\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:274)\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:244)\n\tat org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)\n\tat org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)\n\tat org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:684)\n\tat org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)\n\tat org.glassfish.jersey.servlet.ServletContainer.serviceImpl(ServletContainer.java:378)\n\tat org.glassfish.jersey.servlet.ServletContainer.doFilter(ServletContainer.java:553)\n\tat org.glassfish.jersey.servlet.ServletContainer.doFilter(ServletContainer.java:494)\n\tat org.glassfish.jersey.servlet.ServletContainer.doFilter(ServletContainer.java:431)\n\tat org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n\tat org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:552)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\n\tat org.eclipse.jetty.server.handler.RequestLogHandler.handle(RequestLogHandler.java:54)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n\tat io.confluent.kafka.schemaregistry.rest.RequestIdHandler.handle(RequestIdHandler.java:51)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)\n\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)\n\tat org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:181)\n\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n\tat org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:772)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: io.confluent.kafka.schemaregistry.exceptions.IncompatibleSchemaException: [{errorType:'READER_FIELD_MISSING_DEFAULT_VALUE', description:'The field 'favorite_number' at path '/fields/1' in the new schema has no default value and is missing in the old schema', additionalInfo:'favorite_number'}, {errorType:'READER_FIELD_MISSING_DEFAULT_VALUE', description:'The field 'favorite_color' at path '/fields/2' in the new schema has no default value and is missing in the old schema', additionalInfo:'favorite_color'}, {oldSchemaVersion: 1}, {oldSchema: '{\"type\":\"record\",\"name\":\"User\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"age\",\"type\":\"int\"}]}'}, {compatibility: 'BACKWARD'}]\n\tat io.confluent.kafka.schemaregistry.storage.KafkaSchemaRegistry.register(KafkaSchemaRegistry.java:739)\n\tat io.confluent.kafka.schemaregistry.storage.KafkaSchemaRegistry.registerOrForward(KafkaSchemaRegistry.java:870)\n\tat io.confluent.kafka.schemaregistry.rest.resources.SubjectVersionsResource.register(SubjectVersionsResource.java:412)\n\t... 61 more\n (HTTP status code 409, SR code 409)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from uuid import uuid4\n",
    "\n",
    "from confluent_kafka import Producer\n",
    "from confluent_kafka.serialization import StringSerializer, SerializationContext, MessageField\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n",
    "\n",
    "topic = \"my-topic\"\n",
    "schema = \"user.avsc\"\n",
    "\n",
    "#path = os.path.realpath(os.path.dirname(__file__))\n",
    "with open(f\"{schema}\") as f:\n",
    "    schema_str = f.read()\n",
    "\n",
    "schema_registry_conf = {'url': \"http://localhost:8081/\"}\n",
    "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
    "\n",
    "avro_serializer = AvroSerializer(schema_registry_client,\n",
    "                                    schema_str,\n",
    "                                    user_to_dict)\n",
    "\n",
    "string_serializer = StringSerializer('utf_8')\n",
    "\n",
    "producer_conf = {'bootstrap.servers': 'localhost:9092'}\n",
    "\n",
    "producer = Producer(producer_conf)\n",
    "\n",
    "print(\"Producing user records to topic {}. ^C to exit.\".format(topic))\n",
    "while True:\n",
    "    # Serve on_delivery callbacks from previous calls to produce()\n",
    "    producer.poll(0.0)\n",
    "    try:\n",
    "        user_name = input(\"Enter name: \")\n",
    "        user_address = input(\"Enter address: \")\n",
    "        user_favorite_number = int(input(\"Enter favorite number: \"))\n",
    "        user_favorite_color = input(\"Enter favorite color: \")\n",
    "        user = User(name=user_name,\n",
    "                    address=user_address,\n",
    "                    favorite_color=user_favorite_color,\n",
    "                    favorite_number=user_favorite_number)\n",
    "        producer.produce(topic=topic,\n",
    "                            key=string_serializer(str(uuid4())),\n",
    "                            value=avro_serializer(user, SerializationContext(topic, MessageField.VALUE)),\n",
    "                            on_delivery=delivery_report)\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    except ValueError:\n",
    "        print(\"Invalid input, discarding record...\")\n",
    "        continue\n",
    "\n",
    "print(\"\\nFlushing records...\")\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KAFKA CONNECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "response = requests.get(\"http://localhost:8083/connectors\")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error_code': 404, 'message': 'Connector my_table-connector not found'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(\"http://localhost:8083/connectors/my_table-connector\")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем коннектор для чтения из файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEBEZIUM CDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "pwd = \"secret\"\n",
    "uid = \"debezium\"\n",
    "server = \"localhost\"\n",
    "db = \"mydb\"\n",
    "port = \"5432\"\n",
    "#\n",
    "engine = create_engine(f'postgresql://{uid}:{pwd}@{server}:{port}/{db}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>John Doe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Bob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>John Doe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Bob</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      name\n",
       "0   1  John Doe\n",
       "1   2     Alice\n",
       "2   3       Bob\n",
       "3   4  John Doe\n",
       "4   5     Alice\n",
       "5   6       Bob"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_sql('select * from public.my_table', engine)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создайте DataFrame с данными для вставки\n",
    "data = {'name': ['asd----', 'Alice', 'Bob']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Вставка данных в my_table\n",
    "df.to_sql('my_table', engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'version': '2.6.1',\n",
       " 'commit': '6b2021cd52659cef',\n",
       " 'kafka_cluster_id': 'MkU3OEVBNTcwNTJENDM2Qk'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(\"http://localhost:8083/\")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_connector = {\n",
    "    \"name\": \"my_table-connector\",\n",
    "    \"config\": {\n",
    "        \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\",\n",
    "        \"database.hostname\": \"postgr\",  \n",
    "        \"database.port\": \"5432\",\n",
    "        \"database.user\": \"debezium\",      \n",
    "        \"database.password\": \"secret\", \n",
    "        \"database.dbname\": \"mydb\",  \n",
    "        \"plugin.name\": \"pgoutput\",\n",
    "        \"database.server.name\": \"source\",\n",
    "        \"key.converter.schemas.enable\": \"false\",\n",
    "        \"value.converter.schemas.enable\": \"false\",\n",
    "        \"transforms\": \"unwrap\",\n",
    "        \"transforms.unwrap.type\": \"io.debezium.transforms.ExtractNewRecordState\",\n",
    "        \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n",
    "        \"key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n",
    "        \"table.include.list\": \"public.my_table\", \n",
    "        \"slot.name\": \"dbz_sales_transaction_slot\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get('http://localhost:8083/connectors/')\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка при отправке данных: 201 {\"name\":\"my_table-connector\",\"config\":{\"connector.class\":\"io.debezium.connector.postgresql.PostgresConnector\",\"database.hostname\":\"postgr\",\"database.port\":\"5432\",\"database.user\":\"debezium\",\"database.password\":\"secret\",\"database.dbname\":\"mydb\",\"plugin.name\":\"pgoutput\",\"database.server.name\":\"source\",\"key.converter.schemas.enable\":\"false\",\"value.converter.schemas.enable\":\"false\",\"transforms\":\"unwrap\",\"transforms.unwrap.type\":\"io.debezium.transforms.ExtractNewRecordState\",\"value.converter\":\"org.apache.kafka.connect.json.JsonConverter\",\"key.converter\":\"org.apache.kafka.connect.json.JsonConverter\",\"table.include.list\":\"public.my_table\",\"slot.name\":\"dbz_sales_transaction_slot\",\"name\":\"my_table-connector\"},\"tasks\":[],\"type\":\"source\"}\n"
     ]
    }
   ],
   "source": [
    "url = 'http://localhost:8083/connectors/'\n",
    "\n",
    "\n",
    "# Отправляем данные как JSON\n",
    "response = requests.post(url, json = postgres_connector)\n",
    "\n",
    "# Проверяем статус код ответа\n",
    "if response.status_code == 200:\n",
    "    print(\"Данные успешно отправлены.\")\n",
    "    print(\"Ответ сервера:\", response.json())  # Если сервер возвращает JSON-ответ\n",
    "else:\n",
    "    print(\"Ошибка при отправке данных:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'my_table-connector',\n",
       " 'config': {'connector.class': 'io.debezium.connector.postgresql.PostgresConnector',\n",
       "  'database.user': 'debezium',\n",
       "  'database.dbname': 'mydb',\n",
       "  'slot.name': 'dbz_sales_transaction_slot',\n",
       "  'transforms': 'unwrap',\n",
       "  'database.server.name': 'source',\n",
       "  'database.port': '5432',\n",
       "  'plugin.name': 'pgoutput',\n",
       "  'key.converter.schemas.enable': 'false',\n",
       "  'database.hostname': 'postgr',\n",
       "  'database.password': 'secret',\n",
       "  'value.converter.schemas.enable': 'false',\n",
       "  'name': 'my_table-connector',\n",
       "  'transforms.unwrap.type': 'io.debezium.transforms.ExtractNewRecordState',\n",
       "  'value.converter': 'org.apache.kafka.connect.json.JsonConverter',\n",
       "  'table.include.list': 'public.my_table',\n",
       "  'key.converter': 'org.apache.kafka.connect.json.JsonConverter'},\n",
       " 'tasks': [{'connector': 'my_table-connector', 'task': 0}],\n",
       " 'type': 'source'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get('http://localhost:8083/connectors/my_table-connector')\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'my_table-connector',\n",
       " 'connector': {'state': 'RUNNING', 'worker_id': '172.18.0.5:8083'},\n",
       " 'tasks': [{'id': 0, 'state': 'RUNNING', 'worker_id': '172.18.0.5:8083'}],\n",
       " 'type': 'source'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get('http://localhost:8083/connectors/my_table-connector/status')\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: confluent-kafka\n",
      "Version: 2.6.0\n",
      "Summary: Confluent's Python client for Apache Kafka\n",
      "Home-page: https://github.com/confluentinc/confluent-kafka-python\n",
      "Author: Confluent Inc\n",
      "Author-email: support@confluent.io\n",
      "License: \n",
      "Location: /root/myenv/lib/python3.12/site-packages\n",
      "Requires: \n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show confluent-kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connect_status\n",
      "connect_offsets\n",
      "connect_configs\n",
      "_schemas\n",
      "source.public.my_table\n",
      "__consumer_offsets\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka.admin import AdminClient   \n",
    "\n",
    "# Определите адреса брокеров Kafka\n",
    "bootstrap_servers = ['localhost:9092']\n",
    "\n",
    "# Настройка Kafka\n",
    "config = {\n",
    "    'bootstrap.servers': 'localhost:9092'  # Адрес вашего Kafka-брокера\n",
    "}\n",
    "# Создайте экземпляр KafkaAdminClient\n",
    "admin_client = AdminClient(config)\n",
    "\n",
    "# Запрос метаданных для получения списка топиков\n",
    "metadata = admin_client.list_topics(timeout=10)\n",
    "\n",
    "# Получение списка топиков\n",
    "topics = metadata.topics\n",
    "\n",
    "# Вывод списка топиков\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "bootstrap_servers = ['localhost:29092']\n",
    "consumer = KafkaConsumer( bootstrap_servers=bootstrap_servers)\n",
    "consumer.topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "topicName = 'source.public.my_table'\n",
    "# Initialize consumer variable\n",
    "consumer = KafkaConsumer (topicName , auto_offset_reset='earliest', \n",
    "                          bootstrap_servers = bootstrap_servers, group_id='sales-transactions')\n",
    "\n",
    "# Read and print message from consumer\n",
    "for msg in consumer:\n",
    "    print(json.loads(msg.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление\n",
    "#response = requests.delete('http://localhost:8083/connectors/my_table-connector')\n",
    "#response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received message: я1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received message: qwe\n",
      "Received message: asd\n",
      "Received message: zxc\n",
      "Received message: qwf\n",
      "Received message: asd\n",
      "Received message: hello\n",
      "Received message: gr1\n",
      "Received message: asd\n",
      "Received message: asd\n",
      "Received message: asdzxv\n",
      "Received message: 123\n",
      "Received message: 123\n",
      "Received message: 123\n",
      "Received message: я1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition: 0, Offset: 14\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
