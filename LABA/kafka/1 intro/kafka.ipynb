{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received message: \u0000\u0000\u0000\u0000\u0003\u0006qwered \n",
      "Received message: \u0000\u0000\u0000\u0000\u0003\u0006qwered \n",
      "Received message: \u0000\u0000\u0000\u0000\u0003\u0006qwered \n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Читаем с автоматическим офсетом\n",
    "from confluent_kafka import Consumer, KafkaException\n",
    "# Конфигурация консюмера\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'my-group11wfewf12',\n",
    "    'auto.offset.reset': 'earliest',  # Чтение с самого начала, если офсеты не найдены\n",
    "    'enable.auto.commit': True        # Включение автоматического коммита офсетов\n",
    "}\n",
    "# Создание консюмера\n",
    "consumer = Consumer(conf)\n",
    "# Подписка на топик\n",
    "consumer.subscribe(['my-avro-topic'])\n",
    "try:\n",
    "    for _ in range(5):\n",
    "        msg = consumer.poll(timeout=1.0)  # Ожидание новых сообщений  \n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            raise KafkaException(msg.error())\n",
    "        print(f'Received message: {msg.value().decode(\"utf-8\")}')\n",
    "        \n",
    "finally:\n",
    "    # Закрытие консюмера\n",
    "    consumer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Читаем с РУЧНЫМ офсетом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received message: \u0000\u0000\u0000\u0000\u0003\u0006qwered \n",
      "Received message: \u0000\u0000\u0000\u0000\u0003\u0006qwered \n",
      "Received message: \u0000\u0000\u0000\u0000\u0003\u0006qwered \n",
      "Received message: \u0000\u0000\u0000\u0000\u0003\u0006qwered \n",
      "Received message: \u0000\u0000\u0000\u0000\u0003\u0006qwered \n",
      "Received message: \u0000\u0000\u0000\u0000\u0003\u0006qwered \n",
      "Received message: \u0000\u0000\u0000\u0000\u0003\u0006qwered \n",
      "Received message: \u0000\u0000\u0000\u0000\u0003\u0006qwered \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from confluent_kafka import Consumer, KafkaException\n",
    "# Конфигурация консюмера\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'my-group-222222222', ## другая группа\n",
    "    'auto.offset.reset': 'earliest',  # Чтение с самого начала, если офсеты не найдены\n",
    "    'enable.auto.commit': False        # \n",
    "}\n",
    "# Создание консюмера\n",
    "consumer = Consumer(conf)\n",
    "# Подписка на топик\n",
    "consumer.subscribe(['my-avro-topic'])\n",
    "try:\n",
    "    for _ in range(15):\n",
    "        msg = consumer.poll(timeout=1.0)  # Ожидание новых сообщений  \n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            raise KafkaException(msg.error())\n",
    "        print(f'Received message: {msg.value().decode(\"utf-8\")}')\n",
    "        \n",
    "finally:\n",
    "    # Закрытие консюмера\n",
    "    consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтение офсетов для конкретной группы консумеров\n",
    "Для чтения текущих офсетов для группы консумеров можно использовать метод committed(partitions). Этот метод возвращает список офсетов для указанных разделов (partitions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition: 0, Offset: 9\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Consumer, TopicPartition\n",
    "\n",
    "# Конфигурация консюмера\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'my-group11wfewf',\n",
    "    'auto.offset.reset': 'earliest',\n",
    "    'enable.auto.commit': False\n",
    "}\n",
    "\n",
    "consumer = Consumer(conf)\n",
    "consumer.subscribe(['my-topic'])\n",
    "\n",
    "# Получение текущих офсетов для группы консумеров по разделам топика\n",
    "partitions = [TopicPartition('my-topic', partition=0)]\n",
    "offsets = consumer.committed(partitions)\n",
    "\n",
    "for tp in offsets:\n",
    "    print(f'Partition: {tp.partition}, Offset: {tp.offset}')\n",
    "\n",
    "consumer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message delivered to my-topic [0] at offset 9\n",
      "Message delivered to my-topic [0] at offset 10\n",
      "Message delivered to my-topic [0] at offset 11\n",
      "Message delivered to my-topic [0] at offset 12\n",
      "finally\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Producer\n",
    "\n",
    "# Функция для обработки успешной доставки сообщений\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print(f'Message delivery failed: {err}')\n",
    "    else:\n",
    "        print(f'Message delivered to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}')\n",
    "\n",
    "# Конфигурация продюсера\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',  # Адрес Kafka брокера\n",
    "    'client.id': 'my-producer'                # Идентификатор клиента\n",
    "}\n",
    "\n",
    "# Создание экземпляра продюсера\n",
    "producer = Producer(conf)\n",
    "\n",
    "# Отправка сообщений\n",
    "try:\n",
    "    for i in range(4):\n",
    "        key = f'key-{i}'                # Ключ сообщения (опционально)\n",
    "        value = f'Hello Kafka {i}'      # Значение сообщения\n",
    "        producer.produce('my-topic', key=key, value=value, callback=delivery_report)\n",
    "\n",
    "        # Обработка асинхронных операций\n",
    "        producer.poll(0)  # Проверка на наличие ошибок и вызов обратных функций\n",
    "\n",
    "    # Ожидание отправки всех сообщений\n",
    "    producer.flush()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Error while producing: {e}')\n",
    "\n",
    "finally:\n",
    "    # Закрытие продюсера\n",
    "    print('finally')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exactly-once semantics, EOS\n",
    "\n",
    " idempotent producer и настроек для транзакций\n",
    " Конфигурация продюсера:\n",
    "\n",
    "enable.idempotence: True: Включает идемпотентный режим для продюсера.\n",
    "transactional.id: Уникальный идентификатор для транзакций. Он должен быть уникальным для каждого продюсера в вашем приложении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%4|1729763253.160|GETPID|my-producer#producer-16| [thrd:main]: Failed to acquire transactional PID from broker TxnCoordinator/1: Broker: Not coordinator: retrying\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message delivered to my-topic [0] at offset 13\n",
      "Message delivered to my-topic [0] at offset 14\n",
      "Message delivered to my-topic [0] at offset 15\n",
      "Message delivered to my-topic [0] at offset 16\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Producer\n",
    "\n",
    "# Функция для обработки успешной доставки сообщений\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print(f'Message delivery failed: {err}')\n",
    "    else:\n",
    "        print(f'Message delivered to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}')\n",
    "\n",
    "# Конфигурация продюсера\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',  # Адрес Kafka брокера\n",
    "    'client.id': 'my-producer',              # Идентификатор клиента\n",
    "    'enable.idempotence': True,               # Включение идемпотентного режима\n",
    "    'acks': 'all',                            # Подтверждение от всех реплик\n",
    "    'retries': 5,                             # Количество попыток повторной отправки\n",
    "    'transactional.id': 'my-transactional-id'  # Идентификатор транзакции\n",
    "}\n",
    "\n",
    "# Создание экземпляра продюсера\n",
    "producer = Producer(conf)\n",
    "\n",
    "# Инициализация транзакции\n",
    "producer.init_transactions()\n",
    "\n",
    "try:\n",
    "    # Начало транзакции\n",
    "    producer.begin_transaction()\n",
    "\n",
    "    # Отправка сообщений\n",
    "    for i in range(4):\n",
    "        key = f'key-{i}'                # Ключ сообщения (опционально)\n",
    "        value = f'Hello Kafka {i}'      # Значение сообщения\n",
    "        producer.produce('my-topic', key=key, value=value, callback=delivery_report)\n",
    "\n",
    "        # Обработка асинхронных операций\n",
    "        producer.poll(0)  # Проверка на наличие ошибок и вызов обратных функций\n",
    "\n",
    "    # Завершение транзакции\n",
    "    producer.commit_transaction()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Error while producing: {e}')\n",
    "    # Отмена транзакции в случае ошибки\n",
    "    producer.abort_transaction()\n",
    "\n",
    "finally:\n",
    "    # Ожидание отправки всех сообщений\n",
    "    producer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "response = requests.get(\"http://localhost:8081/subjects\")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка при удалении схемы: 404\n",
      "{\"error_code\":40401,\"message\":\"Subject 'my-avro-topic-value' not found. io.confluent.rest.exceptions.RestNotFoundException: Subject 'my-avro-topic-value' not found.\\nio.confluent.rest.exceptions.RestNotFoundException: Subject 'my-avro-topic-value' not found.\\n\\tat io.confluent.kafka.schemaregistry.rest.exceptions.Errors.subjectNotFoundException(Errors.java:78)\\n\\tat io.confluent.kafka.schemaregistry.rest.resources.SubjectsResource.deleteSubject(SubjectsResource.java:278)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\\n\\tat org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)\\n\\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:134)\\n\\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:177)\\n\\tat org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$VoidOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:159)\\n\\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:81)\\n\\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)\\n\\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)\\n\\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)\\n\\tat org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)\\n\\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)\\n\\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)\\n\\tat org.glassfish.jersey.internal.Errors.process(Errors.java:292)\\n\\tat org.glassfish.jersey.internal.Errors.process(Errors.java:274)\\n\\tat org.glassfish.jersey.internal.Errors.process(Errors.java:244)\\n\\tat org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)\\n\\tat org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)\\n\\tat org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:684)\\n\\tat org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)\\n\\tat org.glassfish.jersey.servlet.ServletContainer.serviceImpl(ServletContainer.java:378)\\n\\tat org.glassfish.jersey.servlet.ServletContainer.doFilter(ServletContainer.java:553)\\n\\tat org.glassfish.jersey.servlet.ServletContainer.doFilter(ServletContainer.java:494)\\n\\tat org.glassfish.jersey.servlet.ServletContainer.doFilter(ServletContainer.java:431)\\n\\tat org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:552)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\\n\\tat org.eclipse.jetty.server.handler.RequestLogHandler.handle(RequestLogHandler.java:54)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\\n\\tat io.confluent.kafka.schemaregistry.rest.RequestIdHandler.handle(RequestIdHandler.java:51)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)\\n\\tat org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:181)\\n\\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\\n\\tat org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:722)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\\n\\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\\n\\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"}\n"
     ]
    }
   ],
   "source": [
    "### УДАЛЕНИЕ\n",
    "import requests\n",
    "\n",
    "# URL для удаления схемы\n",
    "url = 'http://localhost:8081/subjects/my-avro-topic-value'\n",
    "\n",
    "# Отправляем DELETE-запрос\n",
    "response = requests.delete(url)\n",
    "\n",
    "# Проверяем результат\n",
    "if response.status_code == 200:\n",
    "    print(f\"Схема для 'my-topic4-value' успешно удалена.\")\n",
    "else:\n",
    "    print(f\"Ошибка при удалении схемы: {response.status_code}\\n{response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error_code': 40401,\n",
       " 'message': \"Subject 'my-avro-topic-value' not found. io.confluent.rest.exceptions.RestNotFoundException: Subject 'my-avro-topic-value' not found.\\nio.confluent.rest.exceptions.RestNotFoundException: Subject 'my-avro-topic-value' not found.\\n\\tat io.confluent.kafka.schemaregistry.rest.exceptions.Errors.subjectNotFoundException(Errors.java:78)\\n\\tat io.confluent.kafka.schemaregistry.rest.resources.SubjectVersionsResource.getSchemaByVersion(SubjectVersionsResource.java:152)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\\n\\tat org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)\\n\\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:134)\\n\\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:177)\\n\\tat org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$TypeOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:219)\\n\\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:81)\\n\\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)\\n\\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)\\n\\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)\\n\\tat org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)\\n\\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)\\n\\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)\\n\\tat org.glassfish.jersey.internal.Errors.process(Errors.java:292)\\n\\tat org.glassfish.jersey.internal.Errors.process(Errors.java:274)\\n\\tat org.glassfish.jersey.internal.Errors.process(Errors.java:244)\\n\\tat org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)\\n\\tat org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)\\n\\tat org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:684)\\n\\tat org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)\\n\\tat org.glassfish.jersey.servlet.ServletContainer.serviceImpl(ServletContainer.java:378)\\n\\tat org.glassfish.jersey.servlet.ServletContainer.doFilter(ServletContainer.java:553)\\n\\tat org.glassfish.jersey.servlet.ServletContainer.doFilter(ServletContainer.java:494)\\n\\tat org.glassfish.jersey.servlet.ServletContainer.doFilter(ServletContainer.java:431)\\n\\tat org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:552)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\\n\\tat org.eclipse.jetty.server.handler.RequestLogHandler.handle(RequestLogHandler.java:54)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\\n\\tat io.confluent.kafka.schemaregistry.rest.RequestIdHandler.handle(RequestIdHandler.java:51)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)\\n\\tat org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:181)\\n\\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\\n\\tat org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:772)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\\n\\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\\n\\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\\n\\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(\"http://localhost:8081/subjects/my-avro-topic-value/versions/latest\")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регистрация новой схемы\n",
    "регистрируем JSON-схему\n",
    "\n",
    "json_schema = {\n",
    "    \"title\": \"User\",\n",
    "    \"type\": \"object\",\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Схема успешно зарегистрирована: {'id': 7}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Конфигурация Schema Registry\n",
    "schema_registry_url = 'http://localhost:8081'\n",
    "subject = 'my-json-topic-value'  # Название subject для вашей схемы\n",
    "\n",
    "# JSON-схема с массивом пользователей\n",
    "json_schema = {\n",
    "    \"title\": \"UserList\",\n",
    "    \"type\": \"array\",\n",
    "    \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"name\": {\"type\": \"string\"},\n",
    "            \"age\": {\"type\": \"integer\"},\n",
    "            \"email\": {\"type\": \"string\"}\n",
    "        },\n",
    "        \"required\": [\"name\", \"age\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Подготовка данных для регистрации схемы\n",
    "data = {\n",
    "    \"schema\": json.dumps(json_schema),\n",
    "    \"schemaType\": \"JSON\"  # Указываем тип схемы\n",
    "}\n",
    "\n",
    "# Запрос на регистрацию схемы\n",
    "response = requests.post(\n",
    "    f'{schema_registry_url}/subjects/{subject}/versions',\n",
    "    headers={'Content-Type': 'application/json'},\n",
    "    data=json.dumps(data)\n",
    ")\n",
    "\n",
    "# Проверка ответа\n",
    "if response.status_code == 200:\n",
    "    print(f\"Схема успешно зарегистрирована: {response.json()}\")\n",
    "else:\n",
    "    print(f\"Ошибка регистрации схемы: {response.status_code} {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Полученная схема: {\"title\":\"UserList\",\"type\":\"array\",\"items\":{\"type\":\"object\",\"properties\":{\"name\":{\"type\":\"string\"},\"age\":{\"type\":\"integer\"},\"email\":{\"type\":\"string\"}},\"required\":[\"name\",\"age\"]}}\n"
     ]
    }
   ],
   "source": [
    "schema_id = 7  # Замените на нужный ID\n",
    "url = f'http://localhost:8081/schemas/ids/{schema_id}'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    schema = response.json().get('schema')\n",
    "    print('Полученная схема:', schema)\n",
    "else:\n",
    "    print('Ошибка при получении схемы:', response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## пишем JSON используя новую схему"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"title\":\"UserList\",\"type\":\"array\",\"items\":{\"type\":\"object\",\"properties\":{\"name\":{\"type\":\"string\"},\"age\":{\"type\":\"integer\"},\"email\":{\"type\":\"string\"}},\"required\":[\"name\",\"age\"]}}\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Producer\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.json_schema import JSONSerializer\n",
    "from confluent_kafka.serialization import SerializationContext, MessageField\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "# Конфигурация Schema Registry\n",
    "schema_registry_conf = {'url': \"http://localhost:8081\"}\n",
    "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
    "\n",
    "# Получаем последнюю версию схемы из Schema Registry\n",
    "subject = 'my-json-topic-value'  # Субъект (subject), под которым схема зарегистрирована\n",
    "schema_response = schema_registry_client.get_latest_version(subject)\n",
    "schema_id = schema_response.schema_id\n",
    "json_schema_str = schema_response.schema.schema_str\n",
    "print(json_schema_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record b'1bc6fb41-c847-46e1-b214-7228929553d0' successfully produced to my-json-topic [0] at offset 0\n",
      "Message sent successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Создаем JSONSerializer с использованием загруженной схемы\n",
    "json_serializer = JSONSerializer(json_schema_str, schema_registry_client)\n",
    "\n",
    "# Конфигурация продюсера Kafka\n",
    "producer_conf = {\n",
    "    'bootstrap.servers': 'localhost:9092'\n",
    "}\n",
    "producer = Producer(producer_conf)\n",
    "\n",
    "# Топик Kafka\n",
    "topic = 'my-json-topic'\n",
    "\n",
    "# Пример JSON-сообщения, которое соответствует схеме\n",
    "data = [\n",
    "    {\"name\": \"Alice\", \"age\": 30, \"email\": \"alice@example.com\"},\n",
    "    {\"name\": \"Bob\", \"age\": 25, \"email\": \"bob@example.com\"}\n",
    "]\n",
    "\n",
    "# Функция для обратного вызова при доставке сообщения\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print(f\"Delivery failed for record {msg.key()}: {err}\")\n",
    "    else:\n",
    "        print(f\"Record {msg.key()} successfully produced to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}\")\n",
    "\n",
    "# Производство сообщений в топик Kafka\n",
    "try:\n",
    "    key = str(uuid.uuid4())  # Генерация уникального ключа для каждого сообщения\n",
    "    serialized_value = json_serializer(data, SerializationContext(topic, MessageField.VALUE))\n",
    "    \n",
    "    producer.produce(\n",
    "        topic=topic,\n",
    "        key=key,\n",
    "        value=serialized_value,\n",
    "        on_delivery=delivery_report\n",
    "    )\n",
    "    producer.flush()  # Дождаться отправки всех сообщений\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(\"Message sent successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'name' is a required property\n",
      "Message sent successfully.\n"
     ]
    }
   ],
   "source": [
    "## пишем другую схему\n",
    "# Создаем JSONSerializer с использованием загруженной схемы\n",
    "json_serializer = JSONSerializer(json_schema_str, schema_registry_client)\n",
    "\n",
    "# Конфигурация продюсера Kafka\n",
    "producer_conf = {\n",
    "    'bootstrap.servers': 'localhost:9092'\n",
    "}\n",
    "producer = Producer(producer_conf)\n",
    "\n",
    "# Топик Kafka\n",
    "topic = 'my-json-topic'\n",
    "\n",
    "# Пример JSON-сообщения, которое соответствует схеме\n",
    "data = [\n",
    "    {\"name1\": \"Alice\", \"age\": 30, \"email\": \"alice@example.com\"},\n",
    "    {\"name2\": \"Bob\", \"age\": 25, \"email\": \"bob@example.com\"}\n",
    "]\n",
    "\n",
    "# Функция для обратного вызова при доставке сообщения\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print(f\"Delivery failed for record {msg.key()}: {err}\")\n",
    "    else:\n",
    "        print(f\"Record {msg.key()} successfully produced to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}\")\n",
    "\n",
    "# Производство сообщений в топик Kafka\n",
    "try:\n",
    "    key = str(uuid.uuid4())  # Генерация уникального ключа для каждого сообщения\n",
    "    serialized_value = json_serializer(data, SerializationContext(topic, MessageField.VALUE))\n",
    "    \n",
    "    producer.produce(\n",
    "        topic=topic,\n",
    "        key=key,\n",
    "        value=serialized_value,\n",
    "        on_delivery=delivery_report\n",
    "    )\n",
    "    producer.flush()  # Дождаться отправки всех сообщений\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(\"Message sent successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AVRO-схема"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#пробуем зарегистрировать несколько разновидностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Схема зарегистрирована с ID: 8\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = 'http://localhost:8081/subjects/my-topic-value/versions'\n",
    "schema = {\n",
    "    \"schema\": json.dumps({\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"User\",\n",
    "        \"fields\": [\n",
    "            {\"name\": \"name\", \"type\": \"string\"},\n",
    "            {\"name\": \"age\", \"type\": \"int\"}\n",
    "            ,\n",
    "            {\"name\": \"email\", \"type\": \"int\"}#, \"default\": \"\"}\n",
    "        ]\n",
    "    })\n",
    "}\n",
    "## POST-запрос!!!\n",
    "response = requests.post(url, json=schema)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    schema_id = response.json().get('id')\n",
    "    print('Схема зарегистрирована с ID:', schema_id)\n",
    "else:\n",
    "    print('Ошибка при регистрации схемы:', response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(\"http://localhost:8081/subjects/my-topic-value/versions\")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject': 'my-topic-value',\n",
       " 'version': 5,\n",
       " 'id': 8,\n",
       " 'schema': '{\"type\":\"record\",\"name\":\"User\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"age\",\"type\":\"int\"},{\"name\":\"email\",\"type\":\"int\"}]}'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(\"http://localhost:8081/subjects/my-topic-value/versions/latest\")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для AVRO - Консюмер определит версию схемы по магическим байтикам.\n",
    "\n",
    "Для JSON надо делать ручками, например - писать версию в само соообщение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AVRO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avro-файл — это бинарный файл, который используется для хранения данных, сериализованных в формате Avro. Он состоит из двух основных частей:\n",
    "\n",
    "Заголовок (Header): содержит метаданные и схему данных, которая описывает формат записей в файле.\n",
    "Тело (Data Blocks): содержит сами данные (записи), закодированные в соответствии с указанной схемой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Структура Avro-файла\n",
    "# 1. Заголовок (Header)\n",
    "# Каждый Avro-файл начинается с заголовка, который включает в себя:\n",
    "\n",
    "# Магическое число (magic): первые 4 байта файла — это последовательность байт\n",
    "#  0x4F 0x62 0x6A 0x01 (или строка \"Obj\\x01\"). Это используется для идентификации файла \n",
    "# как Avro-файл.\n",
    "#Метаданные (Metadata): хранит JSON-объект с информацией о схеме (например, \"avro.schema\") \n",
    "# и другую служебную информацию.\n",
    "#Синхронизационный маркер (Sync Marker): случайная последовательность из 16 байт, которая используется для синхронизации при чтении файла блоками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## работа с avro \n",
    "import fastavro\n",
    "\n",
    "# Определение схемы\n",
    "schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"User\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"name\", \"type\": \"string\"},\n",
    "        {\"name\": \"age\", \"type\": \"int\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Данные, которые будут сериализованы\n",
    "records = [\n",
    "    {\"name\": \"Alice\", \"age\": 25},\n",
    "    {\"name\": \"Bob\", \"age\": 30}\n",
    "]\n",
    "\n",
    "# Запись данных в Avro-файл\n",
    "with open('test.avro', 'wb') as out:\n",
    "    fastavro.writer(out, schema, records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Alice', 'age': 25}\n",
      "{'name': 'Bob', 'age': 30}\n"
     ]
    }
   ],
   "source": [
    "import fastavro\n",
    "\n",
    "# Чтение Avro-файла\n",
    "with open('test.avro', 'rb') as f:\n",
    "    reader = fastavro.reader(f)\n",
    "    for record in reader:\n",
    "        print(record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11d10a2c-1188-48bf-9540-0a866c711adb'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "str(uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример в kafka_avro_producer.py !\n",
    "======================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening to topic 'my-avro-topic'...\n",
      "Raw key (bytes): b'89cd9d5e-6a08-49e5-bafb-162a56f6f95c'\n",
      "Raw value (bytes): b'\\x00\\x00\\x00\\x00\\x03\\x06qwe\\x02\\x08red '\n",
      "Key (decoded): 89cd9d5e-6a08-49e5-bafb-162a56f6f95c\n",
      "Value (decoded): \u0000\u0000\u0000\u0000\u0003\u0006qwered \n",
      "Raw key (bytes): b'69842775-b927-4ec8-bf3f-8a3a6856de93'\n",
      "Raw value (bytes): b'\\x00\\x00\\x00\\x00\\x03\\x06qwe\\x02\\x08red '\n",
      "Key (decoded): 69842775-b927-4ec8-bf3f-8a3a6856de93\n",
      "Value (decoded): \u0000\u0000\u0000\u0000\u0003\u0006qwered \n",
      "Raw key (bytes): b'15f44a57-c27f-46fe-8079-0a20f7c11b36'\n",
      "Raw value (bytes): b'\\x00\\x00\\x00\\x00\\x03\\x06qwe\\x02\\x08red '\n",
      "Key (decoded): 15f44a57-c27f-46fe-8079-0a20f7c11b36\n",
      "Value (decoded): \u0000\u0000\u0000\u0000\u0003\u0006qwered \n"
     ]
    }
   ],
   "source": [
    "### Читаем сырые байтики\n",
    "from confluent_kafka import Consumer\n",
    "\n",
    "# Конфигурация консьюмера\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'my_group11',\n",
    "    'auto.offset.reset': 'earliest'  # Чтение сообщений с начала, если нет сохранённых оффсетов\n",
    "}\n",
    "\n",
    "# Создаем консьюмера\n",
    "consumer = Consumer(conf)\n",
    "\n",
    "# Подписка на топик\n",
    "topic = \"my-avro-topic\"\n",
    "consumer.subscribe([topic])\n",
    "\n",
    "print(f\"Listening to topic '{topic}'...\")\n",
    "\n",
    "# Чтение сообщений\n",
    "try:\n",
    "    for x in range(10):\n",
    "        msg = consumer.poll(1.0)  # Ожидание сообщений\n",
    "\n",
    "        if msg is None:\n",
    "            continue\n",
    "\n",
    "        if msg.error():\n",
    "            print(f\"Consumer error: {msg.error()}\")\n",
    "            continue\n",
    "\n",
    "        # Получение \"сырых\" данных сообщения\n",
    "        raw_key = msg.key()  # Байты ключа\n",
    "        raw_value = msg.value()  # Байты значения\n",
    "\n",
    "        # Вывод байтов сообщения\n",
    "        print(f\"Raw key (bytes): {raw_key}\")\n",
    "        print(f\"Raw value (bytes): {raw_value}\")\n",
    "\n",
    "        # Если хотите вывести их в текстовом виде (для строк):\n",
    "        print(f\"Key (decoded): {raw_key.decode('utf-8') if raw_key else None}\")\n",
    "        print(f\"Value (decoded): {raw_value.decode('utf-8') if raw_value else None}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Consumer interrupted.\")\n",
    "finally:\n",
    "    consumer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_68507/1827773791.py:13: DeprecationWarning: AvroConsumer has been deprecated. Use AvroDeserializer instead.\n",
      "  consumer = AvroConsumer(conf)\n"
     ]
    },
    {
     "ename": "SerializerError",
     "evalue": "Message deserialization failed for message at my-avro-topic [0] offset 8: message does not start with magic byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSerializerError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/confluent_kafka/avro/__init__.py:184\u001b[0m, in \u001b[0;36mAvroConsumer.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message\u001b[38;5;241m.\u001b[39mkey() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 184\u001b[0m     decoded_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serializer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     message\u001b[38;5;241m.\u001b[39mset_key(decoded_key)\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/confluent_kafka/avro/serializer/message_serializer.py:234\u001b[0m, in \u001b[0;36mMessageSerializer.decode_message\u001b[0;34m(self, message, is_key)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m!=\u001b[39m MAGIC_BYTE:\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SerializerError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage does not start with magic byte\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    235\u001b[0m decoder_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_decoder_func(schema_id, payload, is_key)\n",
      "\u001b[0;31mSerializerError\u001b[0m: message does not start with magic byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSerializerError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 21\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[43mconsumer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/confluent_kafka/avro/__init__.py:187\u001b[0m, in \u001b[0;36mAvroConsumer.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    185\u001b[0m             message\u001b[38;5;241m.\u001b[39mset_key(decoded_key)\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m SerializerError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 187\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SerializerError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage deserialization failed for message at \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m] offset \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    188\u001b[0m             message\u001b[38;5;241m.\u001b[39mtopic(),\n\u001b[1;32m    189\u001b[0m             message\u001b[38;5;241m.\u001b[39mpartition(),\n\u001b[1;32m    190\u001b[0m             message\u001b[38;5;241m.\u001b[39moffset(),\n\u001b[1;32m    191\u001b[0m             e))\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m message\n",
      "\u001b[0;31mSerializerError\u001b[0m: Message deserialization failed for message at my-avro-topic [0] offset 8: message does not start with magic byte"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Consumer\n",
    "from confluent_kafka.avro import AvroConsumer\n",
    "\n",
    "# Конфигурация консьюмера\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'my_group11',\n",
    "    'auto.offset.reset': 'earliest',  # Начинать чтение с самого начала, если нет смещений\n",
    "    'schema.registry.url': 'http://localhost:8081',  # Schema Registry URL\n",
    "}\n",
    "\n",
    "# Создаем Avro-консьюмера\n",
    "consumer = AvroConsumer(conf)\n",
    "\n",
    "# Подписываемся на топик\n",
    "consumer.subscribe(['my-avro-topic'])\n",
    "\n",
    "# Чтение сообщений\n",
    "try:\n",
    "    for x in range(10):\n",
    "        msg = consumer.poll(1.0)\n",
    "\n",
    "        if msg is None:\n",
    "            continue\n",
    "\n",
    "        # Выводим полученное сообщение\n",
    "        print(f\"Получено сообщение: {msg.value()}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    consumer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error_code': 40401,\n",
       " 'message': \"Subject 'my-topic-value' not found. io.confluent.rest.exceptions.RestNotFoundException: Subject 'my-topic-value' not found.\\nio.confluent.rest.exceptions.RestNotFoundException: Subject 'my-topic-value' not found.\\n\\tat io.confluent.kafka.schemaregistry.rest.exceptions.Errors.subjectNotFoundException(Errors.java:78)\\n\\tat io.confluent.kafka.schemaregistry.rest.resources.SubjectVersionsResource.getSchemaByVersion(SubjectVersionsResource.java:152)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\\n\\tat org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)\\n\\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:134)\\n\\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:177)\\n\\tat org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$TypeOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:219)\\n\\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:81)\\n\\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)\\n\\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)\\n\\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)\\n\\tat org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)\\n\\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)\\n\\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)\\n\\tat org.glassfish.jersey.internal.Errors.process(Errors.java:292)\\n\\tat org.glassfish.jersey.internal.Errors.process(Errors.java:274)\\n\\tat org.glassfish.jersey.internal.Errors.process(Errors.java:244)\\n\\tat org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)\\n\\tat org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)\\n\\tat org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:684)\\n\\tat org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)\\n\\tat org.glassfish.jersey.servlet.ServletContainer.serviceImpl(ServletContainer.java:378)\\n\\tat org.glassfish.jersey.servlet.ServletContainer.doFilter(ServletContainer.java:553)\\n\\tat org.glassfish.jersey.servlet.ServletContainer.doFilter(ServletContainer.java:494)\\n\\tat org.glassfish.jersey.servlet.ServletContainer.doFilter(ServletContainer.java:431)\\n\\tat org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:552)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\\n\\tat org.eclipse.jetty.server.handler.RequestLogHandler.handle(RequestLogHandler.java:54)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\\n\\tat io.confluent.kafka.schemaregistry.rest.RequestIdHandler.handle(RequestIdHandler.java:51)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)\\n\\tat org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:181)\\n\\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\\n\\tat org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:772)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\\n\\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\\n\\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\\n\\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(\"http://localhost:8081/subjects/my-topic-value/versions/latest\")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KAFKA CONNECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file-source-connector']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "response = requests.get(\"http://localhost:8083/connectors\")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'file-source-connector',\n",
       " 'config': {'connector.class': 'FileStreamSource',\n",
       "  'file': '/tmp/tmp/test-file.txt',\n",
       "  'tasks.max': '1',\n",
       "  'poll.interval.ms': '1000',\n",
       "  'name': 'file-source-connector',\n",
       "  'topic': 'file-topic'},\n",
       " 'tasks': [{'connector': 'file-source-connector', 'task': 0}],\n",
       " 'type': 'source'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(\"http://localhost:8083/connectors/file-source-connector\")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем коннектор для чтения из файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файловый коннектор успешно создан\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = 'http://localhost:8083/connectors'\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"name\": \"file-source-connector\",  # Имя коннектора\n",
    "    \"config\": {\n",
    "        \"connector.class\": \"FileStreamSource\",  # Класс коннектора\n",
    "        \"tasks.max\": \"1\",  # Количество задач\n",
    "        \"file\": \"/tmp/tmp/test-file.txt\",  # Путь к файлу, который нужно читать\n",
    "        \"topic\": \"file-topic\",  # Топик, куда отправлять данные\n",
    "        \"poll.interval.ms\": \"1000\"  # Интервал опроса файла (в миллисекундах)\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "if response.status_code == 201:\n",
    "    print('Файловый коннектор успешно создан')\n",
    "else:\n",
    "    print('Ошибка при создании файлового коннектора:', response.status_code, response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'file-source-connector',\n",
       " 'connector': {'state': 'RUNNING', 'worker_id': '172.18.0.6:8083'},\n",
       " 'tasks': [{'id': 0, 'state': 'RUNNING', 'worker_id': '172.18.0.6:8083'}],\n",
       " 'type': 'source'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## STATUS\n",
    "## GET /connectors/{connector_name}/status\n",
    "response = requests.get(\"http://localhost:8083/connectors/file-source-connector/status\")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEBEZIUM CDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "pwd = \"secret\"\n",
    "uid = \"debezium\"\n",
    "server = \"localhost\"\n",
    "db = \"mydb\"\n",
    "port = \"5432\"\n",
    "#\n",
    "engine = create_engine(f'postgresql://{uid}:{pwd}@{server}:{port}/{db}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>John Doe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Bob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>John Doe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Bob</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      name\n",
       "0   1  John Doe\n",
       "1   2     Alice\n",
       "2   3       Bob\n",
       "3   4  John Doe\n",
       "4   5     Alice\n",
       "5   6       Bob"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_sql('select * from public.my_table', engine)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создайте DataFrame с данными для вставки\n",
    "data = {'name': ['asd----', 'Alice', 'Bob']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Вставка данных в my_table\n",
    "df.to_sql('my_table', engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'version': '2.6.1',\n",
       " 'commit': '6b2021cd52659cef',\n",
       " 'kafka_cluster_id': 'MkU3OEVBNTcwNTJENDM2Qk'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(\"http://localhost:8083/\")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_connector = {\n",
    "    \"name\": \"my_table-connector\",\n",
    "    \"config\": {\n",
    "        \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\",\n",
    "        \"database.hostname\": \"postgr\",  \n",
    "        \"database.port\": \"5432\",\n",
    "        \"database.user\": \"debezium\",      \n",
    "        \"database.password\": \"secret\", \n",
    "        \"database.dbname\": \"mydb\",  \n",
    "        \"plugin.name\": \"pgoutput\",\n",
    "        \"database.server.name\": \"source\",\n",
    "        \"key.converter.schemas.enable\": \"false\",\n",
    "        \"value.converter.schemas.enable\": \"false\",\n",
    "        \"transforms\": \"unwrap\",\n",
    "        \"transforms.unwrap.type\": \"io.debezium.transforms.ExtractNewRecordState\",\n",
    "        \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n",
    "        \"key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n",
    "        \"table.include.list\": \"public.my_table\", \n",
    "        \"slot.name\": \"dbz_sales_transaction_slot\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file-source-connector']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%3|1729853781.469|FAIL|rdkafka#producer-6| [thrd:185.12.94.232:9092/1]: 185.12.94.232:9092/1: Connect to ipv4#185.12.94.232:9092 failed: Connection refused (after 0ms in state CONNECT)\n",
      "%3|1729853782.470|FAIL|rdkafka#producer-6| [thrd:185.12.94.232:9092/1]: 185.12.94.232:9092/1: Connect to ipv4#185.12.94.232:9092 failed: Connection refused (after 0ms in state CONNECT, 1 identical error(s) suppressed)\n",
      "%3|1729853783.470|FAIL|rdkafka#producer-6| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT)\n",
      "%3|1729853786.471|FAIL|rdkafka#producer-6| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv6#[::1]:9092 failed: Connection refused (after 0ms in state CONNECT)\n",
      "%3|1729853787.471|FAIL|rdkafka#producer-6| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT)\n",
      "%6|1729853792.488|FAIL|rdkafka#producer-6| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Disconnected while requesting ApiVersion: might be caused by incorrect security.protocol configuration (connecting to a SSL listener?) or broker version is < 0.10 (see api.version.request) (after 14ms in state APIVERSION_QUERY)\n",
      "%6|1729853792.674|FAIL|rdkafka#producer-6| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Disconnected while requesting ApiVersion: might be caused by incorrect security.protocol configuration (connecting to a SSL listener?) or broker version is < 0.10 (see api.version.request) (after 1ms in state APIVERSION_QUERY, 1 identical error(s) suppressed)\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('http://localhost:8083/connectors/')\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'postgres_connector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://localhost:8083/connectors/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Отправляем данные как JSON\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(url, json \u001b[38;5;241m=\u001b[39m \u001b[43mpostgres_connector\u001b[49m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Проверяем статус код ответа\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'postgres_connector' is not defined"
     ]
    }
   ],
   "source": [
    "url = 'http://localhost:8083/connectors/'\n",
    "\n",
    "\n",
    "# Отправляем данные как JSON\n",
    "response = requests.post(url, json = postgres_connector)\n",
    "\n",
    "# Проверяем статус код ответа\n",
    "if response.status_code == 200:\n",
    "    print(\"Данные успешно отправлены.\")\n",
    "    print(\"Ответ сервера:\", response.json())  # Если сервер возвращает JSON-ответ\n",
    "else:\n",
    "    print(\"Ошибка при отправке данных:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'my_table-connector',\n",
       " 'config': {'connector.class': 'io.debezium.connector.postgresql.PostgresConnector',\n",
       "  'database.user': 'debezium',\n",
       "  'database.dbname': 'mydb',\n",
       "  'slot.name': 'dbz_sales_transaction_slot',\n",
       "  'transforms': 'unwrap',\n",
       "  'database.server.name': 'source',\n",
       "  'database.port': '5432',\n",
       "  'plugin.name': 'pgoutput',\n",
       "  'key.converter.schemas.enable': 'false',\n",
       "  'database.hostname': 'postgr',\n",
       "  'database.password': 'secret',\n",
       "  'value.converter.schemas.enable': 'false',\n",
       "  'name': 'my_table-connector',\n",
       "  'transforms.unwrap.type': 'io.debezium.transforms.ExtractNewRecordState',\n",
       "  'value.converter': 'org.apache.kafka.connect.json.JsonConverter',\n",
       "  'table.include.list': 'public.my_table',\n",
       "  'key.converter': 'org.apache.kafka.connect.json.JsonConverter'},\n",
       " 'tasks': [{'connector': 'my_table-connector', 'task': 0}],\n",
       " 'type': 'source'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get('http://localhost:8083/connectors/my_table-connector')\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'my_table-connector',\n",
       " 'connector': {'state': 'RUNNING', 'worker_id': '172.18.0.5:8083'},\n",
       " 'tasks': [{'id': 0, 'state': 'RUNNING', 'worker_id': '172.18.0.5:8083'}],\n",
       " 'type': 'source'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get('http://localhost:8083/connectors/my_table-connector/status')\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: confluent-kafka\n",
      "Version: 2.6.0\n",
      "Summary: Confluent's Python client for Apache Kafka\n",
      "Home-page: https://github.com/confluentinc/confluent-kafka-python\n",
      "Author: Confluent Inc\n",
      "Author-email: support@confluent.io\n",
      "License: \n",
      "Location: /root/myenv/lib/python3.12/site-packages\n",
      "Requires: \n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show confluent-kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connect_status\n",
      "connect_offsets\n",
      "connect_configs\n",
      "_schemas\n",
      "source.public.my_table\n",
      "__consumer_offsets\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka.admin import AdminClient   \n",
    "\n",
    "# Определите адреса брокеров Kafka\n",
    "bootstrap_servers = ['localhost:9092']\n",
    "\n",
    "# Настройка Kafka\n",
    "config = {\n",
    "    'bootstrap.servers': 'localhost:9092'  # Адрес вашего Kafka-брокера\n",
    "}\n",
    "# Создайте экземпляр KafkaAdminClient\n",
    "admin_client = AdminClient(config)\n",
    "\n",
    "# Запрос метаданных для получения списка топиков\n",
    "metadata = admin_client.list_topics(timeout=10)\n",
    "\n",
    "# Получение списка топиков\n",
    "topics = metadata.topics\n",
    "\n",
    "# Вывод списка топиков\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "bootstrap_servers = ['localhost:29092']\n",
    "consumer = KafkaConsumer( bootstrap_servers=bootstrap_servers)\n",
    "consumer.topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "topicName = 'source.public.my_table'\n",
    "# Initialize consumer variable\n",
    "consumer = KafkaConsumer (topicName , auto_offset_reset='earliest', \n",
    "                          bootstrap_servers = bootstrap_servers, group_id='sales-transactions')\n",
    "\n",
    "# Read and print message from consumer\n",
    "for msg in consumer:\n",
    "    print(json.loads(msg.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление\n",
    "#response = requests.delete('http://localhost:8083/connectors/my_table-connector')\n",
    "#response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
