{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Читаем с автоматическим офсетом\n",
    "from confluent_kafka import Consumer, KafkaException\n",
    "# Конфигурация консюмера\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'my-group11',\n",
    "    'auto.offset.reset': 'earliest',  # Чтение с самого начала, если офсеты не найдены\n",
    "    'enable.auto.commit': True        # Включение автоматического коммита офсетов\n",
    "}\n",
    "# Создание консюмера\n",
    "consumer = Consumer(conf)\n",
    "# Подписка на топик\n",
    "consumer.subscribe(['my-topic'])\n",
    "try:\n",
    "    for _ in range(5):\n",
    "        msg = consumer.poll(timeout=1.0)  # Ожидание новых сообщений  \n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            raise KafkaException(msg.error())\n",
    "        print(f'Received message: {msg.value().decode(\"utf-8\")}')\n",
    "        \n",
    "finally:\n",
    "    # Закрытие консюмера\n",
    "    consumer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Читаем с РУЧНЫМ офсетом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received message: 123\n",
      "Received message: 456\n",
      "Received message: 789\n",
      "Received message: 000\n",
      "Received message: 111\n",
      "Received message: 22222\n",
      "Received message: 3333\n",
      "Received message: aaa\n",
      "Received message: zzzzzz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from confluent_kafka import Consumer, KafkaException\n",
    "# Конфигурация консюмера\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'my-group-222222222', ## другая группа\n",
    "    'auto.offset.reset': 'earliest',  # Чтение с самого начала, если офсеты не найдены\n",
    "    'enable.auto.commit': False        # \n",
    "}\n",
    "# Создание консюмера\n",
    "consumer = Consumer(conf)\n",
    "# Подписка на топик\n",
    "consumer.subscribe(['my-topic'])\n",
    "try:\n",
    "    for _ in range(15):\n",
    "        msg = consumer.poll(timeout=1.0)  # Ожидание новых сообщений  \n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            raise KafkaException(msg.error())\n",
    "        print(f'Received message: {msg.value().decode(\"utf-8\")}')\n",
    "        \n",
    "finally:\n",
    "    # Закрытие консюмера\n",
    "    consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтение офсетов для конкретной группы консумеров\n",
    "Для чтения текущих офсетов для группы консумеров можно использовать метод committed(partitions). Этот метод возвращает список офсетов для указанных разделов (partitions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition: 0, Offset: 9\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Consumer, TopicPartition\n",
    "\n",
    "# Конфигурация консюмера\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'my-group',\n",
    "    'auto.offset.reset': 'earliest',\n",
    "    'enable.auto.commit': False\n",
    "}\n",
    "\n",
    "consumer = Consumer(conf)\n",
    "consumer.subscribe(['my-topic'])\n",
    "\n",
    "# Получение текущих офсетов для группы консумеров по разделам топика\n",
    "partitions = [TopicPartition('my-topic', partition=0)]\n",
    "offsets = consumer.committed(partitions)\n",
    "\n",
    "for tp in offsets:\n",
    "    print(f'Partition: {tp.partition}, Offset: {tp.offset}')\n",
    "\n",
    "consumer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message delivered to my-topic [0] at offset 39\n",
      "Message delivered to my-topic [0] at offset 40\n",
      "Message delivered to my-topic [0] at offset 41\n",
      "Message delivered to my-topic [0] at offset 42\n",
      "finally\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Producer\n",
    "\n",
    "# Функция для обработки успешной доставки сообщений\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print(f'Message delivery failed: {err}')\n",
    "    else:\n",
    "        print(f'Message delivered to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}')\n",
    "\n",
    "# Конфигурация продюсера\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',  # Адрес Kafka брокера\n",
    "    'client.id': 'my-producer'                # Идентификатор клиента\n",
    "}\n",
    "\n",
    "# Создание экземпляра продюсера\n",
    "producer = Producer(conf)\n",
    "\n",
    "# Отправка сообщений\n",
    "try:\n",
    "    for i in range(4):\n",
    "        key = f'key-{i}'                # Ключ сообщения (опционально)\n",
    "        value = f'Hello Kafka {i}'      # Значение сообщения\n",
    "        producer.produce('my-topic', key=key, value=value, callback=delivery_report)\n",
    "\n",
    "        # Обработка асинхронных операций\n",
    "        producer.poll(0)  # Проверка на наличие ошибок и вызов обратных функций\n",
    "\n",
    "    # Ожидание отправки всех сообщений\n",
    "    producer.flush()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Error while producing: {e}')\n",
    "\n",
    "finally:\n",
    "    # Закрытие продюсера\n",
    "    print('finally')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exactly-once semantics, EOS\n",
    "\n",
    " idempotent producer и настроек для транзакций\n",
    " Конфигурация продюсера:\n",
    "\n",
    "enable.idempotence: True: Включает идемпотентный режим для продюсера.\n",
    "transactional.id: Уникальный идентификатор для транзакций. Он должен быть уникальным для каждого продюсера в вашем приложении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%4|1729585021.552|GETPID|my-producer#producer-22| [thrd:main]: Failed to acquire transactional PID from broker TxnCoordinator/1: Broker: Not coordinator: retrying\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message delivered to my-topic [0] at offset 34\n",
      "Message delivered to my-topic [0] at offset 35\n",
      "Message delivered to my-topic [0] at offset 36\n",
      "Message delivered to my-topic [0] at offset 37\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Producer\n",
    "\n",
    "# Функция для обработки успешной доставки сообщений\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print(f'Message delivery failed: {err}')\n",
    "    else:\n",
    "        print(f'Message delivered to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}')\n",
    "\n",
    "# Конфигурация продюсера\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',  # Адрес Kafka брокера\n",
    "    'client.id': 'my-producer',              # Идентификатор клиента\n",
    "    'enable.idempotence': True,               # Включение идемпотентного режима\n",
    "    'acks': 'all',                            # Подтверждение от всех реплик\n",
    "    'retries': 5,                             # Количество попыток повторной отправки\n",
    "    'transactional.id': 'my-transactional-id'  # Идентификатор транзакции\n",
    "}\n",
    "\n",
    "# Создание экземпляра продюсера\n",
    "producer = Producer(conf)\n",
    "\n",
    "# Инициализация транзакции\n",
    "producer.init_transactions()\n",
    "\n",
    "try:\n",
    "    # Начало транзакции\n",
    "    producer.begin_transaction()\n",
    "\n",
    "    # Отправка сообщений\n",
    "    for i in range(4):\n",
    "        key = f'key-{i}'                # Ключ сообщения (опционально)\n",
    "        value = f'Hello Kafka {i}'      # Значение сообщения\n",
    "        producer.produce('my-topic', key=key, value=value, callback=delivery_report)\n",
    "\n",
    "        # Обработка асинхронных операций\n",
    "        producer.poll(0)  # Проверка на наличие ошибок и вызов обратных функций\n",
    "\n",
    "    # Завершение транзакции\n",
    "    producer.commit_transaction()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Error while producing: {e}')\n",
    "    # Отмена транзакции в случае ошибки\n",
    "    producer.abort_transaction()\n",
    "\n",
    "finally:\n",
    "    # Ожидание отправки всех сообщений\n",
    "    producer.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file-topic-value']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "response = requests.get(\"http://localhost:8081/subjects\")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject': 'file-topic-value', 'version': 1, 'id': 1, 'schema': '\"string\"'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(\"http://localhost:8081/subjects/file-topic-value/versions/latest\")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регистрация новой схемы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка при регистрации схемы: {\"error_code\":409,\"message\":\"Schema being registered is incompatible with an earlier schema for subject \\\"file-topic-value\\\", details: [{errorType:'TYPE_MISMATCH', description:'The type (path '/') of a field in the new schema does not match with the old schema', additionalInfo:'reader type: RECORD not compatible with writer type: STRING'}, {oldSchemaVersion: 1}, {oldSchema: '\\\"string\\\"'}, {compatibility: 'BACKWARD'}] io.confluent.kafka.schemaregistry.rest.exceptions.RestIncompatibleSchemaException: Schema being registered is incompatible with an earlier schema for subject \\\"file-topic-value\\\", details: [{errorType:'TYPE_MISMATCH', description:'The type (path '/') of a field in the new schema does not match with the old schema', additionalInfo:'reader type: RECORD not compatible with writer type: STRING'}, {oldSchemaVersion: 1}, {oldSchema: '\\\"string\\\"'}, {compatibility: 'BACKWARD'}]\\nio.confluent.kafka.schemaregistry.rest.exceptions.RestIncompatibleSchemaException: Schema being registered is incompatible with an earlier schema for subject \\\"file-topic-value\\\", details: [{errorType:'TYPE_MISMATCH', description:'The type (path '/') of a field in the new schema does not match with the old schema', additionalInfo:'reader type: RECORD not compatible with writer type: STRING'}, {oldSchemaVersion: 1}, {oldSchema: '\\\"string\\\"'}, {compatibility: 'BACKWARD'}]\\n\\tat io.confluent.kafka.schemaregistry.rest.exceptions.Errors.incompatibleSchemaException(Errors.java:134)\\n\\tat io.confluent.kafka.schemaregistry.rest.resources.SubjectVersionsResource.register(SubjectVersionsResource.java:431)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\\n\\tat org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)\\n\\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:134)\\n\\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:177)\\n\\tat org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$VoidOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:159)\\n\\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:81)\\n\\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)\\n\\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)\\n\\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)\\n\\tat org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)\\n\\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)\\n\\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)\\n\\tat org.glassfish.jersey.internal.Errors.process(Errors.java:292)\\n\\tat org.glassfish.jersey.internal.Errors.process(Errors.java:274)\\n\\tat org.glassfish.jersey.internal.Errors.process(Errors.java:244)\\n\\tat org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)\\n\\tat org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)\\n\\tat org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:684)\\n\\tat org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)\\n\\tat org.glassfish.jersey.servlet.ServletContainer.serviceImpl(ServletContainer.java:378)\\n\\tat org.glassfish.jersey.servlet.ServletContainer.doFilter(ServletContainer.java:553)\\n\\tat org.glassfish.jersey.servlet.ServletContainer.doFilter(ServletContainer.java:494)\\n\\tat org.glassfish.jersey.servlet.ServletContainer.doFilter(ServletContainer.java:431)\\n\\tat org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:552)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\\n\\tat org.eclipse.jetty.server.handler.RequestLogHandler.handle(RequestLogHandler.java:54)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\\n\\tat io.confluent.kafka.schemaregistry.rest.RequestIdHandler.handle(RequestIdHandler.java:51)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)\\n\\tat org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:181)\\n\\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\\n\\tat org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:772)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\\n\\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\\n\\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\\n\\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\nCaused by: io.confluent.kafka.schemaregistry.exceptions.IncompatibleSchemaException: [{errorType:'TYPE_MISMATCH', description:'The type (path '/') of a field in the new schema does not match with the old schema', additionalInfo:'reader type: RECORD not compatible with writer type: STRING'}, {oldSchemaVersion: 1}, {oldSchema: '\\\"string\\\"'}, {compatibility: 'BACKWARD'}]\\n\\tat io.confluent.kafka.schemaregistry.storage.KafkaSchemaRegistry.register(KafkaSchemaRegistry.java:739)\\n\\tat io.confluent.kafka.schemaregistry.storage.KafkaSchemaRegistry.registerOrForward(KafkaSchemaRegistry.java:870)\\n\\tat io.confluent.kafka.schemaregistry.rest.resources.SubjectVersionsResource.register(SubjectVersionsResource.java:412)\\n\\t... 66 more\\n\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = 'http://localhost:8081/subjects/my-topic-value/versions'\n",
    "schema = {\n",
    "    \"schema\": json.dumps({\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"User\",\n",
    "        \"fields\": [\n",
    "            {\"name\": \"name\", \"type\": \"string\"},\n",
    "            {\"name\": \"age\", \"type\": \"int\"}\n",
    "        ]\n",
    "    })\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=schema)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    schema_id = response.json().get('id')\n",
    "    print('Схема зарегистрирована с ID:', schema_id)\n",
    "else:\n",
    "    print('Ошибка при регистрации схемы:', response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Полученная схема: {\"type\":\"record\",\"name\":\"User\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"age\",\"type\":\"int\"}]}\n"
     ]
    }
   ],
   "source": [
    "schema_id = 1  # Замените на нужный ID\n",
    "url = f'http://localhost:8081/schemas/ids/{schema_id}'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    schema = response.json().get('schema')\n",
    "    print('Полученная схема:', schema)\n",
    "else:\n",
    "    print('Ошибка при получении схемы:', response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AVRO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avro-файл — это бинарный файл, который используется для хранения данных, сериализованных в формате Avro. Он состоит из двух основных частей:\n",
    "\n",
    "Заголовок (Header): содержит метаданные и схему данных, которая описывает формат записей в файле.\n",
    "Тело (Data Blocks): содержит сами данные (записи), закодированные в соответствии с указанной схемой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Структура Avro-файла\n",
    "# 1. Заголовок (Header)\n",
    "# Каждый Avro-файл начинается с заголовка, который включает в себя:\n",
    "\n",
    "# Магическое число (magic): первые 4 байта файла — это последовательность байт\n",
    "#  0x4F 0x62 0x6A 0x01 (или строка \"Obj\\x01\"). Это используется для идентификации файла \n",
    "# как Avro-файл.\n",
    "#Метаданные (Metadata): хранит JSON-объект с информацией о схеме (например, \"avro.schema\") \n",
    "# и другую служебную информацию.\n",
    "#Синхронизационный маркер (Sync Marker): случайная последовательность из 16 байт, которая используется для синхронизации при чтении файла блоками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## работа с avro \n",
    "import fastavro\n",
    "\n",
    "# Определение схемы\n",
    "schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"User\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"name\", \"type\": \"string\"},\n",
    "        {\"name\": \"age\", \"type\": \"int\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Данные, которые будут сериализованы\n",
    "records = [\n",
    "    {\"name\": \"Alice\", \"age\": 25},\n",
    "    {\"name\": \"Bob\", \"age\": 30}\n",
    "]\n",
    "\n",
    "# Запись данных в Avro-файл\n",
    "with open('test.avro', 'wb') as out:\n",
    "    fastavro.writer(out, schema, records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Alice', 'age': 25}\n",
      "{'name': 'Bob', 'age': 30}\n"
     ]
    }
   ],
   "source": [
    "import fastavro\n",
    "\n",
    "# Чтение Avro-файла\n",
    "with open('test.avro', 'rb') as f:\n",
    "    reader = fastavro.reader(f)\n",
    "    for record in reader:\n",
    "        print(record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Пример в kafka.py!!!!!!!!!!!!!!!1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error_code': 40401,\n",
       " 'message': \"Subject 'my-topic-value' not found. io.confluent.rest.exceptions.RestNotFoundException: Subject 'my-topic-value' not found.\\nio.confluent.rest.exceptions.RestNotFoundException: Subject 'my-topic-value' not found.\\n\\tat io.confluent.kafka.schemaregistry.rest.exceptions.Errors.subjectNotFoundException(Errors.java:78)\\n\\tat io.confluent.kafka.schemaregistry.rest.resources.SubjectVersionsResource.getSchemaByVersion(SubjectVersionsResource.java:152)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\\n\\tat org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)\\n\\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:134)\\n\\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:177)\\n\\tat org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$TypeOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:219)\\n\\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:81)\\n\\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)\\n\\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)\\n\\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)\\n\\tat org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)\\n\\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)\\n\\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)\\n\\tat org.glassfish.jersey.internal.Errors.process(Errors.java:292)\\n\\tat org.glassfish.jersey.internal.Errors.process(Errors.java:274)\\n\\tat org.glassfish.jersey.internal.Errors.process(Errors.java:244)\\n\\tat org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)\\n\\tat org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)\\n\\tat org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:684)\\n\\tat org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)\\n\\tat org.glassfish.jersey.servlet.ServletContainer.serviceImpl(ServletContainer.java:378)\\n\\tat org.glassfish.jersey.servlet.ServletContainer.doFilter(ServletContainer.java:553)\\n\\tat org.glassfish.jersey.servlet.ServletContainer.doFilter(ServletContainer.java:494)\\n\\tat org.glassfish.jersey.servlet.ServletContainer.doFilter(ServletContainer.java:431)\\n\\tat org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:552)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\\n\\tat org.eclipse.jetty.server.handler.RequestLogHandler.handle(RequestLogHandler.java:54)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\\n\\tat io.confluent.kafka.schemaregistry.rest.RequestIdHandler.handle(RequestIdHandler.java:51)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)\\n\\tat org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:181)\\n\\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\\n\\tat org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:772)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\\n\\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\\n\\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(\"http://localhost:8081/subjects/my-topic-value/versions/latest\")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KAFKA CONNECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "response = requests.get(\"http://localhost:8083/connectors\")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error_code': 404, 'message': 'Connector file-source-connector not found'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(\"http://localhost:8083/connectors/file-source-connector\")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем коннектор для чтения из файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файловый коннектор успешно создан\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = 'http://localhost:8083/connectors'\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"name\": \"file-source-connector\",  # Имя коннектора\n",
    "    \"config\": {\n",
    "        \"connector.class\": \"FileStreamSource\",  # Класс коннектора\n",
    "        \"tasks.max\": \"1\",  # Количество задач\n",
    "        \"file\": \"/tmp/tmp/test-file.txt\",  # Путь к файлу, который нужно читать\n",
    "        \"topic\": \"file-topic\",  # Топик, куда отправлять данные\n",
    "        \"poll.interval.ms\": \"1000\"  # Интервал опроса файла (в миллисекундах)\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "if response.status_code == 201:\n",
    "    print('Файловый коннектор успешно создан')\n",
    "else:\n",
    "    print('Ошибка при создании файлового коннектора:', response.status_code, response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEBEZIUM CDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "pwd = \"secret\"\n",
    "uid = \"debezium\"\n",
    "server = \"localhost\"\n",
    "db = \"mydb\"\n",
    "port = \"5432\"\n",
    "#\n",
    "engine = create_engine(f'postgresql://{uid}:{pwd}@{server}:{port}/{db}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>John Doe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Bob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>John Doe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Bob</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      name\n",
       "0   1  John Doe\n",
       "1   2     Alice\n",
       "2   3       Bob\n",
       "3   4  John Doe\n",
       "4   5     Alice\n",
       "5   6       Bob"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_sql('select * from public.my_table', engine)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создайте DataFrame с данными для вставки\n",
    "data = {'name': ['asd----', 'Alice', 'Bob']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Вставка данных в my_table\n",
    "df.to_sql('my_table', engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'version': '2.6.1',\n",
       " 'commit': '6b2021cd52659cef',\n",
       " 'kafka_cluster_id': 'MkU3OEVBNTcwNTJENDM2Qk'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(\"http://localhost:8083/\")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_connector = {\n",
    "    \"name\": \"my_table-connector\",\n",
    "    \"config\": {\n",
    "        \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\",\n",
    "        \"database.hostname\": \"postgr\",  \n",
    "        \"database.port\": \"5432\",\n",
    "        \"database.user\": \"debezium\",      \n",
    "        \"database.password\": \"secret\", \n",
    "        \"database.dbname\": \"mydb\",  \n",
    "        \"plugin.name\": \"pgoutput\",\n",
    "        \"database.server.name\": \"source\",\n",
    "        \"key.converter.schemas.enable\": \"false\",\n",
    "        \"value.converter.schemas.enable\": \"false\",\n",
    "        \"transforms\": \"unwrap\",\n",
    "        \"transforms.unwrap.type\": \"io.debezium.transforms.ExtractNewRecordState\",\n",
    "        \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n",
    "        \"key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n",
    "        \"table.include.list\": \"public.my_table\", \n",
    "        \"slot.name\": \"dbz_sales_transaction_slot\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get('http://localhost:8083/connectors/')\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка при отправке данных: 201 {\"name\":\"my_table-connector\",\"config\":{\"connector.class\":\"io.debezium.connector.postgresql.PostgresConnector\",\"database.hostname\":\"postgr\",\"database.port\":\"5432\",\"database.user\":\"debezium\",\"database.password\":\"secret\",\"database.dbname\":\"mydb\",\"plugin.name\":\"pgoutput\",\"database.server.name\":\"source\",\"key.converter.schemas.enable\":\"false\",\"value.converter.schemas.enable\":\"false\",\"transforms\":\"unwrap\",\"transforms.unwrap.type\":\"io.debezium.transforms.ExtractNewRecordState\",\"value.converter\":\"org.apache.kafka.connect.json.JsonConverter\",\"key.converter\":\"org.apache.kafka.connect.json.JsonConverter\",\"table.include.list\":\"public.my_table\",\"slot.name\":\"dbz_sales_transaction_slot\",\"name\":\"my_table-connector\"},\"tasks\":[],\"type\":\"source\"}\n"
     ]
    }
   ],
   "source": [
    "url = 'http://localhost:8083/connectors/'\n",
    "\n",
    "\n",
    "# Отправляем данные как JSON\n",
    "response = requests.post(url, json = postgres_connector)\n",
    "\n",
    "# Проверяем статус код ответа\n",
    "if response.status_code == 200:\n",
    "    print(\"Данные успешно отправлены.\")\n",
    "    print(\"Ответ сервера:\", response.json())  # Если сервер возвращает JSON-ответ\n",
    "else:\n",
    "    print(\"Ошибка при отправке данных:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'my_table-connector',\n",
       " 'config': {'connector.class': 'io.debezium.connector.postgresql.PostgresConnector',\n",
       "  'database.user': 'debezium',\n",
       "  'database.dbname': 'mydb',\n",
       "  'slot.name': 'dbz_sales_transaction_slot',\n",
       "  'transforms': 'unwrap',\n",
       "  'database.server.name': 'source',\n",
       "  'database.port': '5432',\n",
       "  'plugin.name': 'pgoutput',\n",
       "  'key.converter.schemas.enable': 'false',\n",
       "  'database.hostname': 'postgr',\n",
       "  'database.password': 'secret',\n",
       "  'value.converter.schemas.enable': 'false',\n",
       "  'name': 'my_table-connector',\n",
       "  'transforms.unwrap.type': 'io.debezium.transforms.ExtractNewRecordState',\n",
       "  'value.converter': 'org.apache.kafka.connect.json.JsonConverter',\n",
       "  'table.include.list': 'public.my_table',\n",
       "  'key.converter': 'org.apache.kafka.connect.json.JsonConverter'},\n",
       " 'tasks': [{'connector': 'my_table-connector', 'task': 0}],\n",
       " 'type': 'source'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get('http://localhost:8083/connectors/my_table-connector')\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'my_table-connector',\n",
       " 'connector': {'state': 'RUNNING', 'worker_id': '172.18.0.5:8083'},\n",
       " 'tasks': [{'id': 0, 'state': 'RUNNING', 'worker_id': '172.18.0.5:8083'}],\n",
       " 'type': 'source'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get('http://localhost:8083/connectors/my_table-connector/status')\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: confluent-kafka\n",
      "Version: 2.6.0\n",
      "Summary: Confluent's Python client for Apache Kafka\n",
      "Home-page: https://github.com/confluentinc/confluent-kafka-python\n",
      "Author: Confluent Inc\n",
      "Author-email: support@confluent.io\n",
      "License: \n",
      "Location: /root/myenv/lib/python3.12/site-packages\n",
      "Requires: \n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show confluent-kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connect_status\n",
      "connect_offsets\n",
      "connect_configs\n",
      "_schemas\n",
      "source.public.my_table\n",
      "__consumer_offsets\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka.admin import AdminClient   \n",
    "\n",
    "# Определите адреса брокеров Kafka\n",
    "bootstrap_servers = ['localhost:9092']\n",
    "\n",
    "# Настройка Kafka\n",
    "config = {\n",
    "    'bootstrap.servers': 'localhost:9092'  # Адрес вашего Kafka-брокера\n",
    "}\n",
    "# Создайте экземпляр KafkaAdminClient\n",
    "admin_client = AdminClient(config)\n",
    "\n",
    "# Запрос метаданных для получения списка топиков\n",
    "metadata = admin_client.list_topics(timeout=10)\n",
    "\n",
    "# Получение списка топиков\n",
    "topics = metadata.topics\n",
    "\n",
    "# Вывод списка топиков\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "bootstrap_servers = ['localhost:29092']\n",
    "consumer = KafkaConsumer( bootstrap_servers=bootstrap_servers)\n",
    "consumer.topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "topicName = 'source.public.my_table'\n",
    "# Initialize consumer variable\n",
    "consumer = KafkaConsumer (topicName , auto_offset_reset='earliest', \n",
    "                          bootstrap_servers = bootstrap_servers, group_id='sales-transactions')\n",
    "\n",
    "# Read and print message from consumer\n",
    "for msg in consumer:\n",
    "    print(json.loads(msg.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление\n",
    "#response = requests.delete('http://localhost:8083/connectors/my_table-connector')\n",
    "#response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
